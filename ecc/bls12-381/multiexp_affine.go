// Copyright 2020 ConsenSys Software Inc.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// Code generated by consensys/gnark-crypto DO NOT EDIT

package bls12381

import (
	"errors"
	"github.com/consensys/gnark-crypto/ecc"
	"github.com/consensys/gnark-crypto/ecc/bls12-381/fr"
	"math"
	"runtime"
)

const MAX_BATCH_SIZE = 600

type batchOp struct {
	bucketID, pointID uint32
}

func (o batchOp) isNeg() bool {
	return o.pointID&1 == 1
}

// MultiExpBatchAffine implements section 4 of https://eprint.iacr.org/2012/549.pdf
//
// This call return an error if len(scalars) != len(points) or if provided config is invalid.
func (p *G1Affine) MultiExpBatchAffine(points []G1Affine, scalars []fr.Element, config ecc.MultiExpConfig) (*G1Affine, error) {
	var _p G1Jac
	if _, err := _p.MultiExpBatchAffine(points, scalars, config); err != nil {
		return nil, err
	}
	p.FromJacobian(&_p)
	return p, nil
}

// MultiExpBatchAffine implements section 4 of https://eprint.iacr.org/2012/549.pdf
//
// This call return an error if len(scalars) != len(points) or if provided config is invalid.
func (p *G1Jac) MultiExpBatchAffine(points []G1Affine, scalars []fr.Element, config ecc.MultiExpConfig) (*G1Jac, error) {
	// note:
	// each of the batchAffineMsmCX method is the same, except for the c constant it declares
	// duplicating (through template generation) these methods allows to declare the buckets on the stack
	// the choice of c needs to be improved:
	// there is a theoritical value that gives optimal asymptotics
	// but in practice, other factors come into play, including:
	// * if c doesn't divide 64, the word size, then we're bound to select bits over 2 words of our scalars, instead of 1
	// * number of CPUs
	// * cache friendliness (which depends on the host, G1 or G2... )
	//	--> for example, on BN254, a G1 point fits into one cache line of 64bytes, but a G2 point don't.

	// for each batchAffineMsmCX
	// step 1
	// we compute, for each scalars over c-bit wide windows, nbChunk digits
	// if the digit is larger than 2^{c-1}, then, we borrow 2^c from the next window and substract
	// 2^{c} to the current digit, making it negative.
	// negative digits will be processed in the next step as adding -G into the bucket instead of G
	// (computing -G is cheap, and this saves us half of the buckets)
	// step 2
	// buckets are declared on the stack
	// notice that we have 2^{c-1} buckets instead of 2^{c} (see step1)
	// we use jacobian extended formulas here as they are faster than mixed addition
	// msmProcessChunk places points into buckets base on their selector and return the weighted bucket sum in given channel
	// step 3
	// reduce the buckets weigthed sums into our result (msmReduceChunk)

	// ensure len(points) == len(scalars)
	nbPoints := len(points)
	if nbPoints != len(scalars) {
		return nil, errors.New("len(points) != len(scalars)")
	}

	// if nbTasks is not set, use all available CPUs
	if config.NbTasks <= 0 {
		config.NbTasks = runtime.NumCPU()
	} else if config.NbTasks > 1024 {
		return nil, errors.New("invalid config: config.NbTasks > 1024")
	}

	// here, we compute the best C for nbPoints
	// we split recursively until nbChunks(c) >= nbTasks,
	bestC := func(nbPoints int) uint64 {
		// implemented batchAffineMsmC methods (the c we use must be in this slice)
		implementedCs := []uint64{4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 20, 21}
		var C uint64
		// approximate cost (in group operations)
		// cost = bits/c * (nbPoints + 2^{c})
		// this needs to be verified empirically.
		// for example, on a MBP 2016, for G2 MultiExp > 8M points, hand picking c gives better results
		min := math.MaxFloat64
		for _, c := range implementedCs {
			cc := fr.Limbs * 64 * (nbPoints + (1 << (c)))
			cost := float64(cc) / float64(c)
			if cost < min {
				min = cost
				C = c
			}
		}
		// empirical, needs to be tuned.
		// if C > 16 && nbPoints < 1 << 23 {
		// 	C = 16
		// }
		return C
	}

	var C uint64
	nbSplits := 1
	nbChunks := 0
	for nbChunks < config.NbTasks {
		C = bestC(nbPoints)
		nbChunks = int(fr.Limbs * 64 / C) // number of c-bit radixes in a scalar
		if (fr.Limbs*64)%C != 0 {
			nbChunks++
		}
		nbChunks *= nbSplits
		if nbChunks < config.NbTasks {
			nbSplits <<= 1
			nbPoints >>= 1
		}
	}

	// partition the scalars
	// note: we do that before the actual chunk processing, as for each c-bit window (starting from LSW)
	// if it's larger than 2^{c-1}, we have a carry we need to propagate up to the higher window
	var smallValues int
	scalars, smallValues = partitionScalars(scalars, C, config.ScalarsMont, config.NbTasks)

	// if we have more than 10% of small values, we split the processing of the first chunk in 2
	// we may want to do that in msmInnerG1JacBatchAffine , but that would incur a cost of looping through all scalars one more time
	splitFirstChunk := (float64(smallValues) / float64(len(scalars))) >= 0.1

	// we have nbSplits intermediate results that we must sum together.
	_p := make([]G1Jac, nbSplits-1)
	chDone := make(chan int, nbSplits-1)
	for i := 0; i < nbSplits-1; i++ {
		start := i * nbPoints
		end := start + nbPoints
		go func(start, end, i int) {
			msmInnerG1JacBatchAffine(&_p[i], int(C), points[start:end], scalars[start:end], splitFirstChunk)
			chDone <- i
		}(start, end, i)
	}

	msmInnerG1JacBatchAffine(p, int(C), points[(nbSplits-1)*nbPoints:], scalars[(nbSplits-1)*nbPoints:], splitFirstChunk)
	for i := 0; i < nbSplits-1; i++ {
		done := <-chDone
		p.AddAssign(&_p[done])
	}
	close(chDone)
	return p, nil
}

func msmInnerG1JacBatchAffine(p *G1Jac, c int, points []G1Affine, scalars []fr.Element, splitFirstChunk bool) {

	switch c {

	case 4:
		p.msmC4(points, scalars, splitFirstChunk)

	case 5:
		p.msmC5(points, scalars, splitFirstChunk)

	case 6:
		p.msmC6(points, scalars, splitFirstChunk)

	case 7:
		p.msmC7(points, scalars, splitFirstChunk)

	case 8:
		p.msmC8(points, scalars, splitFirstChunk)

	case 9:
		p.msmC9(points, scalars, splitFirstChunk)

	case 10:
		p.batchAffineMsmC10(points, scalars, splitFirstChunk)

	case 11:
		p.batchAffineMsmC11(points, scalars, splitFirstChunk)

	case 12:
		p.batchAffineMsmC12(points, scalars, splitFirstChunk)

	case 13:
		p.batchAffineMsmC13(points, scalars, splitFirstChunk)

	case 14:
		p.batchAffineMsmC14(points, scalars, splitFirstChunk)

	case 15:
		p.batchAffineMsmC15(points, scalars, splitFirstChunk)

	case 16:
		p.batchAffineMsmC16(points, scalars, splitFirstChunk)

	case 20:
		p.batchAffineMsmC20(points, scalars, splitFirstChunk)

	case 21:
		p.batchAffineMsmC21(points, scalars, splitFirstChunk)

	default:
		panic("not implemented")
	}
}

// msmReduceChunkG1AffineBatchAffine reduces the weighted sum of the buckets into the result of the multiExp
func msmReduceChunkG1AffineBatchAffine(p *G1Jac, c int, chChunks []chan g1JacExtended) *G1Jac {
	var _p g1JacExtended
	totalj := <-chChunks[len(chChunks)-1]
	_p.Set(&totalj)
	for j := len(chChunks) - 2; j >= 0; j-- {
		for l := 0; l < c; l++ {
			_p.double(&_p)
		}
		totalj := <-chChunks[j]
		_p.add(&totalj)
	}

	return p.unsafeFromJacExtended(&_p)
}

type BatchG1Affine struct {
	P               [MAX_BATCH_SIZE]G1Affine
	R               [MAX_BATCH_SIZE]*G1Affine
	batchSize       int
	cptP            int
	bucketIds       map[uint32]struct{}
	buckets, points []G1Affine
}

func newBatchG1Affine(buckets, points []G1Affine) BatchG1Affine {
	batchSize := len(buckets) / 5
	if batchSize > MAX_BATCH_SIZE {
		batchSize = MAX_BATCH_SIZE
	}
	if batchSize <= 0 {
		batchSize = 1
	}
	return BatchG1Affine{
		buckets:   buckets,
		points:    points,
		batchSize: batchSize,
		bucketIds: make(map[uint32]struct{}, len(buckets)/2),
	}
}

func (b *BatchG1Affine) IsFull() bool {
	return b.cptP == b.batchSize
}

func (b *BatchG1Affine) ExecuteAndReset() {
	if b.cptP == 0 {
		return
	}
	// for i := 0; i < len(b.R); i++ {
	// 	b.R[i].Add(b.R[i], b.P[i])
	// }
	BatchAddG1Affine(b.R[:b.cptP], b.P[:b.cptP], b.cptP)
	for k := range b.bucketIds {
		delete(b.bucketIds, k)
	}
	// b.bucketIds = [MAX_BATCH_SIZE]uint32{}
	b.cptP = 0
}

func (b *BatchG1Affine) CanAdd(bID uint32) bool {
	_, ok := b.bucketIds[bID]
	return !ok
}

func (b *BatchG1Affine) Add(op batchOp) {
	// CanAdd must be called before --> ensures bucket is not "used" in current batch

	B := &b.buckets[op.bucketID]
	P := &b.points[op.pointID>>1]
	if P.IsInfinity() {
		return
	}
	// handle special cases with inf or -P / P
	if B.IsInfinity() {
		if op.isNeg() {
			B.Neg(P)
		} else {
			B.Set(P)
		}
		return
	}
	if op.isNeg() {
		// if bucket == P --> -P == 0
		if B.Equal(P) {
			B.setInfinity()
			return
		}
	} else {
		// if bucket == -P, B == 0
		if B.X.Equal(&P.X) && !B.Y.Equal(&P.Y) {
			B.setInfinity()
			return
		}
	}

	// b.bucketIds[b.cptP] = op.bucketID
	b.bucketIds[op.bucketID] = struct{}{}
	b.R[b.cptP] = B
	if op.isNeg() {
		b.P[b.cptP].Neg(P)
	} else {
		b.P[b.cptP].Set(P)
	}
	b.cptP++
}

func processQueueG1Affine(queue []batchOp, batch *BatchG1Affine) []batchOp {
	for i := len(queue) - 1; i >= 0; i-- {
		if batch.CanAdd(queue[i].bucketID) {
			batch.Add(queue[i])
			if batch.IsFull() {
				batch.ExecuteAndReset()
			}
			queue[i] = queue[len(queue)-1]
			queue = queue[:len(queue)-1]
		}
	}
	return queue

}

func msmProcessChunkG1AffineBatchAffine(chunk uint64,
	chRes chan<- g1JacExtended,
	buckets []G1Affine,
	c uint64,
	points []G1Affine,
	scalars []fr.Element) {

	mask := uint64((1 << c) - 1) // low c bits are 1
	msbWindow := uint64(1 << (c - 1))

	for i := 0; i < len(buckets); i++ {
		buckets[i].setInfinity()
	}

	jc := uint64(chunk * c)
	s := selector{}
	s.index = jc / 64
	s.shift = jc - (s.index * 64)
	s.mask = mask << s.shift
	s.multiWordSelect = (64%c) != 0 && s.shift > (64-c) && s.index < (fr.Limbs-1)
	if s.multiWordSelect {
		nbBitsHigh := s.shift - uint64(64-c)
		s.maskHigh = (1 << nbBitsHigh) - 1
		s.shiftHigh = (c - nbBitsHigh)
	}

	batch := newBatchG1Affine(buckets, points)
	queue := make([]batchOp, 0, 4096) // TODO find right capacity here.
	nbBatches := 0
	for i := 0; i < len(scalars); i++ {
		bits := (scalars[i][s.index] & s.mask) >> s.shift
		if s.multiWordSelect {
			bits += (scalars[i][s.index+1] & s.maskHigh) << s.shiftHigh
		}

		if bits == 0 {
			continue
		}

		op := batchOp{pointID: uint32(i) << 1}
		// if msbWindow bit is set, we need to substract
		if bits&msbWindow == 0 {
			// add
			op.bucketID = uint32(bits - 1)
			// buckets[bits-1].Add(&points[i], &buckets[bits-1])
		} else {
			// sub
			op.bucketID = (uint32(bits & ^msbWindow))
			op.pointID += 1
			// op.isNeg = true
			// buckets[bits & ^msbWindow].Sub( &buckets[bits & ^msbWindow], &points[i])
		}
		if batch.CanAdd(op.bucketID) {
			batch.Add(op)
			if batch.IsFull() {
				batch.ExecuteAndReset()
				nbBatches++
				if len(queue) != 0 { // TODO @gbotrel this doesn't seem to help much? should minimize queue resizing
					batch.Add(queue[len(queue)-1])
					queue = queue[:len(queue)-1]
				}
			}
		} else {
			// put it in queue.
			queue = append(queue, op)
		}
	}
	// fmt.Printf("chunk %d\nlen(queue)=%d\nnbBatches=%d\nbatchSize=%d\nnbBuckets=%d\nnbPoints=%d\n",
	// 	chunk, len(queue), nbBatches, batch.batchSize, len(buckets), len(points))
	// batch.ExecuteAndReset()
	for len(queue) != 0 {
		queue = processQueueG1Affine(queue, &batch)
		batch.ExecuteAndReset() // execute batch even if not full.
	}

	// flush items in batch.
	batch.ExecuteAndReset()

	// reduce buckets into total
	// total =  bucket[0] + 2*bucket[1] + 3*bucket[2] ... + n*bucket[n-1]

	var runningSum, total g1JacExtended
	runningSum.setInfinity()
	total.setInfinity()
	for k := len(buckets) - 1; k >= 0; k-- {
		if !buckets[k].IsInfinity() {
			runningSum.addMixed(&buckets[k])
		}
		total.add(&runningSum)
	}

	chRes <- total

}

func (p *G1Jac) batchAffineMsmC10(points []G1Affine, scalars []fr.Element, splitFirstChunk bool) *G1Jac {
	const (
		c        = 10                  // scalars partitioned into c-bit radixes
		nbChunks = (fr.Limbs * 64 / c) // number of c-bit radixes in a scalar
	)

	// for each chunk, spawn one go routine that'll loop through all the scalars in the
	// corresponding bit-window
	// note that buckets is an array allocated on the stack (for most sizes of c) and this is
	// critical for performance

	// each go routine sends its result in chChunks[i] channel
	var chChunks [nbChunks + 1]chan g1JacExtended
	for i := 0; i < len(chChunks); i++ {
		chChunks[i] = make(chan g1JacExtended, 1)
	}

	// c doesn't divide 256, last window is smaller we can allocate less buckets
	const lastC = (fr.Limbs * 64) - (c * (fr.Limbs * 64 / c))
	// TODO @gbotrel replace this in code generator
	if lastC >= 10 {
		go func(j uint64, points []G1Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]G1Affine
			msmProcessChunkG1AffineBatchAffine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	} else {
		go func(j uint64, points []G1Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]g1JacExtended
			msmProcessChunkG1Affine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	}

	processChunk := func(j int, points []G1Affine, scalars []fr.Element, chChunk chan g1JacExtended) {
		var buckets [1 << (c - 1)]G1Affine
		msmProcessChunkG1AffineBatchAffine(uint64(j), chChunk, buckets[:], c, points, scalars)
	}

	for j := int(nbChunks - 1); j > 0; j-- {
		go processChunk(j, points, scalars, chChunks[j])
	}

	if !splitFirstChunk {
		go processChunk(0, points, scalars, chChunks[0])
	} else {
		chSplit := make(chan g1JacExtended, 2)
		split := len(points) / 2
		go processChunk(0, points[:split], scalars[:split], chSplit)
		go processChunk(0, points[split:], scalars[split:], chSplit)
		go func() {
			s1 := <-chSplit
			s2 := <-chSplit
			close(chSplit)
			s1.add(&s2)
			chChunks[0] <- s1
		}()
	}

	return msmReduceChunkG1AffineBatchAffine(p, c, chChunks[:])
}

func (p *G1Jac) batchAffineMsmC11(points []G1Affine, scalars []fr.Element, splitFirstChunk bool) *G1Jac {
	const (
		c        = 11                  // scalars partitioned into c-bit radixes
		nbChunks = (fr.Limbs * 64 / c) // number of c-bit radixes in a scalar
	)

	// for each chunk, spawn one go routine that'll loop through all the scalars in the
	// corresponding bit-window
	// note that buckets is an array allocated on the stack (for most sizes of c) and this is
	// critical for performance

	// each go routine sends its result in chChunks[i] channel
	var chChunks [nbChunks + 1]chan g1JacExtended
	for i := 0; i < len(chChunks); i++ {
		chChunks[i] = make(chan g1JacExtended, 1)
	}

	// c doesn't divide 256, last window is smaller we can allocate less buckets
	const lastC = (fr.Limbs * 64) - (c * (fr.Limbs * 64 / c))
	// TODO @gbotrel replace this in code generator
	if lastC >= 10 {
		go func(j uint64, points []G1Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]G1Affine
			msmProcessChunkG1AffineBatchAffine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	} else {
		go func(j uint64, points []G1Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]g1JacExtended
			msmProcessChunkG1Affine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	}

	processChunk := func(j int, points []G1Affine, scalars []fr.Element, chChunk chan g1JacExtended) {
		var buckets [1 << (c - 1)]G1Affine
		msmProcessChunkG1AffineBatchAffine(uint64(j), chChunk, buckets[:], c, points, scalars)
	}

	for j := int(nbChunks - 1); j > 0; j-- {
		go processChunk(j, points, scalars, chChunks[j])
	}

	if !splitFirstChunk {
		go processChunk(0, points, scalars, chChunks[0])
	} else {
		chSplit := make(chan g1JacExtended, 2)
		split := len(points) / 2
		go processChunk(0, points[:split], scalars[:split], chSplit)
		go processChunk(0, points[split:], scalars[split:], chSplit)
		go func() {
			s1 := <-chSplit
			s2 := <-chSplit
			close(chSplit)
			s1.add(&s2)
			chChunks[0] <- s1
		}()
	}

	return msmReduceChunkG1AffineBatchAffine(p, c, chChunks[:])
}

func (p *G1Jac) batchAffineMsmC12(points []G1Affine, scalars []fr.Element, splitFirstChunk bool) *G1Jac {
	const (
		c        = 12                  // scalars partitioned into c-bit radixes
		nbChunks = (fr.Limbs * 64 / c) // number of c-bit radixes in a scalar
	)

	// for each chunk, spawn one go routine that'll loop through all the scalars in the
	// corresponding bit-window
	// note that buckets is an array allocated on the stack (for most sizes of c) and this is
	// critical for performance

	// each go routine sends its result in chChunks[i] channel
	var chChunks [nbChunks + 1]chan g1JacExtended
	for i := 0; i < len(chChunks); i++ {
		chChunks[i] = make(chan g1JacExtended, 1)
	}

	// c doesn't divide 256, last window is smaller we can allocate less buckets
	const lastC = (fr.Limbs * 64) - (c * (fr.Limbs * 64 / c))
	// TODO @gbotrel replace this in code generator
	if lastC >= 10 {
		go func(j uint64, points []G1Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]G1Affine
			msmProcessChunkG1AffineBatchAffine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	} else {
		go func(j uint64, points []G1Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]g1JacExtended
			msmProcessChunkG1Affine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	}

	processChunk := func(j int, points []G1Affine, scalars []fr.Element, chChunk chan g1JacExtended) {
		var buckets [1 << (c - 1)]G1Affine
		msmProcessChunkG1AffineBatchAffine(uint64(j), chChunk, buckets[:], c, points, scalars)
	}

	for j := int(nbChunks - 1); j > 0; j-- {
		go processChunk(j, points, scalars, chChunks[j])
	}

	if !splitFirstChunk {
		go processChunk(0, points, scalars, chChunks[0])
	} else {
		chSplit := make(chan g1JacExtended, 2)
		split := len(points) / 2
		go processChunk(0, points[:split], scalars[:split], chSplit)
		go processChunk(0, points[split:], scalars[split:], chSplit)
		go func() {
			s1 := <-chSplit
			s2 := <-chSplit
			close(chSplit)
			s1.add(&s2)
			chChunks[0] <- s1
		}()
	}

	return msmReduceChunkG1AffineBatchAffine(p, c, chChunks[:])
}

func (p *G1Jac) batchAffineMsmC13(points []G1Affine, scalars []fr.Element, splitFirstChunk bool) *G1Jac {
	const (
		c        = 13                  // scalars partitioned into c-bit radixes
		nbChunks = (fr.Limbs * 64 / c) // number of c-bit radixes in a scalar
	)

	// for each chunk, spawn one go routine that'll loop through all the scalars in the
	// corresponding bit-window
	// note that buckets is an array allocated on the stack (for most sizes of c) and this is
	// critical for performance

	// each go routine sends its result in chChunks[i] channel
	var chChunks [nbChunks + 1]chan g1JacExtended
	for i := 0; i < len(chChunks); i++ {
		chChunks[i] = make(chan g1JacExtended, 1)
	}

	// c doesn't divide 256, last window is smaller we can allocate less buckets
	const lastC = (fr.Limbs * 64) - (c * (fr.Limbs * 64 / c))
	// TODO @gbotrel replace this in code generator
	if lastC >= 10 {
		go func(j uint64, points []G1Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]G1Affine
			msmProcessChunkG1AffineBatchAffine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	} else {
		go func(j uint64, points []G1Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]g1JacExtended
			msmProcessChunkG1Affine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	}

	processChunk := func(j int, points []G1Affine, scalars []fr.Element, chChunk chan g1JacExtended) {
		var buckets [1 << (c - 1)]G1Affine
		msmProcessChunkG1AffineBatchAffine(uint64(j), chChunk, buckets[:], c, points, scalars)
	}

	for j := int(nbChunks - 1); j > 0; j-- {
		go processChunk(j, points, scalars, chChunks[j])
	}

	if !splitFirstChunk {
		go processChunk(0, points, scalars, chChunks[0])
	} else {
		chSplit := make(chan g1JacExtended, 2)
		split := len(points) / 2
		go processChunk(0, points[:split], scalars[:split], chSplit)
		go processChunk(0, points[split:], scalars[split:], chSplit)
		go func() {
			s1 := <-chSplit
			s2 := <-chSplit
			close(chSplit)
			s1.add(&s2)
			chChunks[0] <- s1
		}()
	}

	return msmReduceChunkG1AffineBatchAffine(p, c, chChunks[:])
}

func (p *G1Jac) batchAffineMsmC14(points []G1Affine, scalars []fr.Element, splitFirstChunk bool) *G1Jac {
	const (
		c        = 14                  // scalars partitioned into c-bit radixes
		nbChunks = (fr.Limbs * 64 / c) // number of c-bit radixes in a scalar
	)

	// for each chunk, spawn one go routine that'll loop through all the scalars in the
	// corresponding bit-window
	// note that buckets is an array allocated on the stack (for most sizes of c) and this is
	// critical for performance

	// each go routine sends its result in chChunks[i] channel
	var chChunks [nbChunks + 1]chan g1JacExtended
	for i := 0; i < len(chChunks); i++ {
		chChunks[i] = make(chan g1JacExtended, 1)
	}

	// c doesn't divide 256, last window is smaller we can allocate less buckets
	const lastC = (fr.Limbs * 64) - (c * (fr.Limbs * 64 / c))
	// TODO @gbotrel replace this in code generator
	if lastC >= 10 {
		go func(j uint64, points []G1Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]G1Affine
			msmProcessChunkG1AffineBatchAffine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	} else {
		go func(j uint64, points []G1Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]g1JacExtended
			msmProcessChunkG1Affine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	}

	processChunk := func(j int, points []G1Affine, scalars []fr.Element, chChunk chan g1JacExtended) {
		var buckets [1 << (c - 1)]G1Affine
		msmProcessChunkG1AffineBatchAffine(uint64(j), chChunk, buckets[:], c, points, scalars)
	}

	for j := int(nbChunks - 1); j > 0; j-- {
		go processChunk(j, points, scalars, chChunks[j])
	}

	if !splitFirstChunk {
		go processChunk(0, points, scalars, chChunks[0])
	} else {
		chSplit := make(chan g1JacExtended, 2)
		split := len(points) / 2
		go processChunk(0, points[:split], scalars[:split], chSplit)
		go processChunk(0, points[split:], scalars[split:], chSplit)
		go func() {
			s1 := <-chSplit
			s2 := <-chSplit
			close(chSplit)
			s1.add(&s2)
			chChunks[0] <- s1
		}()
	}

	return msmReduceChunkG1AffineBatchAffine(p, c, chChunks[:])
}

func (p *G1Jac) batchAffineMsmC15(points []G1Affine, scalars []fr.Element, splitFirstChunk bool) *G1Jac {
	const (
		c        = 15                  // scalars partitioned into c-bit radixes
		nbChunks = (fr.Limbs * 64 / c) // number of c-bit radixes in a scalar
	)

	// for each chunk, spawn one go routine that'll loop through all the scalars in the
	// corresponding bit-window
	// note that buckets is an array allocated on the stack (for most sizes of c) and this is
	// critical for performance

	// each go routine sends its result in chChunks[i] channel
	var chChunks [nbChunks + 1]chan g1JacExtended
	for i := 0; i < len(chChunks); i++ {
		chChunks[i] = make(chan g1JacExtended, 1)
	}

	// c doesn't divide 256, last window is smaller we can allocate less buckets
	const lastC = (fr.Limbs * 64) - (c * (fr.Limbs * 64 / c))
	// TODO @gbotrel replace this in code generator
	if lastC >= 10 {
		go func(j uint64, points []G1Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]G1Affine
			msmProcessChunkG1AffineBatchAffine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	} else {
		go func(j uint64, points []G1Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]g1JacExtended
			msmProcessChunkG1Affine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	}

	processChunk := func(j int, points []G1Affine, scalars []fr.Element, chChunk chan g1JacExtended) {
		var buckets [1 << (c - 1)]G1Affine
		msmProcessChunkG1AffineBatchAffine(uint64(j), chChunk, buckets[:], c, points, scalars)
	}

	for j := int(nbChunks - 1); j > 0; j-- {
		go processChunk(j, points, scalars, chChunks[j])
	}

	if !splitFirstChunk {
		go processChunk(0, points, scalars, chChunks[0])
	} else {
		chSplit := make(chan g1JacExtended, 2)
		split := len(points) / 2
		go processChunk(0, points[:split], scalars[:split], chSplit)
		go processChunk(0, points[split:], scalars[split:], chSplit)
		go func() {
			s1 := <-chSplit
			s2 := <-chSplit
			close(chSplit)
			s1.add(&s2)
			chChunks[0] <- s1
		}()
	}

	return msmReduceChunkG1AffineBatchAffine(p, c, chChunks[:])
}

func (p *G1Jac) batchAffineMsmC16(points []G1Affine, scalars []fr.Element, splitFirstChunk bool) *G1Jac {
	const (
		c        = 16                  // scalars partitioned into c-bit radixes
		nbChunks = (fr.Limbs * 64 / c) // number of c-bit radixes in a scalar
	)

	// for each chunk, spawn one go routine that'll loop through all the scalars in the
	// corresponding bit-window
	// note that buckets is an array allocated on the stack (for most sizes of c) and this is
	// critical for performance

	// each go routine sends its result in chChunks[i] channel
	var chChunks [nbChunks]chan g1JacExtended
	for i := 0; i < len(chChunks); i++ {
		chChunks[i] = make(chan g1JacExtended, 1)
	}

	processChunk := func(j int, points []G1Affine, scalars []fr.Element, chChunk chan g1JacExtended) {
		var buckets [1 << (c - 1)]G1Affine
		msmProcessChunkG1AffineBatchAffine(uint64(j), chChunk, buckets[:], c, points, scalars)
	}

	for j := int(nbChunks - 1); j > 0; j-- {
		go processChunk(j, points, scalars, chChunks[j])
	}

	if !splitFirstChunk {
		go processChunk(0, points, scalars, chChunks[0])
	} else {
		chSplit := make(chan g1JacExtended, 2)
		split := len(points) / 2
		go processChunk(0, points[:split], scalars[:split], chSplit)
		go processChunk(0, points[split:], scalars[split:], chSplit)
		go func() {
			s1 := <-chSplit
			s2 := <-chSplit
			close(chSplit)
			s1.add(&s2)
			chChunks[0] <- s1
		}()
	}

	return msmReduceChunkG1AffineBatchAffine(p, c, chChunks[:])
}

func (p *G1Jac) batchAffineMsmC20(points []G1Affine, scalars []fr.Element, splitFirstChunk bool) *G1Jac {
	const (
		c        = 20                  // scalars partitioned into c-bit radixes
		nbChunks = (fr.Limbs * 64 / c) // number of c-bit radixes in a scalar
	)

	// for each chunk, spawn one go routine that'll loop through all the scalars in the
	// corresponding bit-window
	// note that buckets is an array allocated on the stack (for most sizes of c) and this is
	// critical for performance

	// each go routine sends its result in chChunks[i] channel
	var chChunks [nbChunks + 1]chan g1JacExtended
	for i := 0; i < len(chChunks); i++ {
		chChunks[i] = make(chan g1JacExtended, 1)
	}

	// c doesn't divide 256, last window is smaller we can allocate less buckets
	const lastC = (fr.Limbs * 64) - (c * (fr.Limbs * 64 / c))
	// TODO @gbotrel replace this in code generator
	if lastC >= 10 {
		go func(j uint64, points []G1Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]G1Affine
			msmProcessChunkG1AffineBatchAffine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	} else {
		go func(j uint64, points []G1Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]g1JacExtended
			msmProcessChunkG1Affine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	}

	processChunk := func(j int, points []G1Affine, scalars []fr.Element, chChunk chan g1JacExtended) {
		var buckets [1 << (c - 1)]G1Affine
		msmProcessChunkG1AffineBatchAffine(uint64(j), chChunk, buckets[:], c, points, scalars)
	}

	for j := int(nbChunks - 1); j > 0; j-- {
		go processChunk(j, points, scalars, chChunks[j])
	}

	if !splitFirstChunk {
		go processChunk(0, points, scalars, chChunks[0])
	} else {
		chSplit := make(chan g1JacExtended, 2)
		split := len(points) / 2
		go processChunk(0, points[:split], scalars[:split], chSplit)
		go processChunk(0, points[split:], scalars[split:], chSplit)
		go func() {
			s1 := <-chSplit
			s2 := <-chSplit
			close(chSplit)
			s1.add(&s2)
			chChunks[0] <- s1
		}()
	}

	return msmReduceChunkG1AffineBatchAffine(p, c, chChunks[:])
}

func (p *G1Jac) batchAffineMsmC21(points []G1Affine, scalars []fr.Element, splitFirstChunk bool) *G1Jac {
	const (
		c        = 21                  // scalars partitioned into c-bit radixes
		nbChunks = (fr.Limbs * 64 / c) // number of c-bit radixes in a scalar
	)

	// for each chunk, spawn one go routine that'll loop through all the scalars in the
	// corresponding bit-window
	// note that buckets is an array allocated on the stack (for most sizes of c) and this is
	// critical for performance

	// each go routine sends its result in chChunks[i] channel
	var chChunks [nbChunks + 1]chan g1JacExtended
	for i := 0; i < len(chChunks); i++ {
		chChunks[i] = make(chan g1JacExtended, 1)
	}

	// c doesn't divide 256, last window is smaller we can allocate less buckets
	const lastC = (fr.Limbs * 64) - (c * (fr.Limbs * 64 / c))
	// TODO @gbotrel replace this in code generator
	if lastC >= 10 {
		go func(j uint64, points []G1Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]G1Affine
			msmProcessChunkG1AffineBatchAffine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	} else {
		go func(j uint64, points []G1Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]g1JacExtended
			msmProcessChunkG1Affine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	}

	processChunk := func(j int, points []G1Affine, scalars []fr.Element, chChunk chan g1JacExtended) {
		var buckets [1 << (c - 1)]G1Affine
		msmProcessChunkG1AffineBatchAffine(uint64(j), chChunk, buckets[:], c, points, scalars)
	}

	for j := int(nbChunks - 1); j > 0; j-- {
		go processChunk(j, points, scalars, chChunks[j])
	}

	if !splitFirstChunk {
		go processChunk(0, points, scalars, chChunks[0])
	} else {
		chSplit := make(chan g1JacExtended, 2)
		split := len(points) / 2
		go processChunk(0, points[:split], scalars[:split], chSplit)
		go processChunk(0, points[split:], scalars[split:], chSplit)
		go func() {
			s1 := <-chSplit
			s2 := <-chSplit
			close(chSplit)
			s1.add(&s2)
			chChunks[0] <- s1
		}()
	}

	return msmReduceChunkG1AffineBatchAffine(p, c, chChunks[:])
}

// MultiExpBatchAffine implements section 4 of https://eprint.iacr.org/2012/549.pdf
//
// This call return an error if len(scalars) != len(points) or if provided config is invalid.
func (p *G2Affine) MultiExpBatchAffine(points []G2Affine, scalars []fr.Element, config ecc.MultiExpConfig) (*G2Affine, error) {
	var _p G2Jac
	if _, err := _p.MultiExpBatchAffine(points, scalars, config); err != nil {
		return nil, err
	}
	p.FromJacobian(&_p)
	return p, nil
}

// MultiExpBatchAffine implements section 4 of https://eprint.iacr.org/2012/549.pdf
//
// This call return an error if len(scalars) != len(points) or if provided config is invalid.
func (p *G2Jac) MultiExpBatchAffine(points []G2Affine, scalars []fr.Element, config ecc.MultiExpConfig) (*G2Jac, error) {
	// note:
	// each of the batchAffineMsmCX method is the same, except for the c constant it declares
	// duplicating (through template generation) these methods allows to declare the buckets on the stack
	// the choice of c needs to be improved:
	// there is a theoritical value that gives optimal asymptotics
	// but in practice, other factors come into play, including:
	// * if c doesn't divide 64, the word size, then we're bound to select bits over 2 words of our scalars, instead of 1
	// * number of CPUs
	// * cache friendliness (which depends on the host, G1 or G2... )
	//	--> for example, on BN254, a G1 point fits into one cache line of 64bytes, but a G2 point don't.

	// for each batchAffineMsmCX
	// step 1
	// we compute, for each scalars over c-bit wide windows, nbChunk digits
	// if the digit is larger than 2^{c-1}, then, we borrow 2^c from the next window and substract
	// 2^{c} to the current digit, making it negative.
	// negative digits will be processed in the next step as adding -G into the bucket instead of G
	// (computing -G is cheap, and this saves us half of the buckets)
	// step 2
	// buckets are declared on the stack
	// notice that we have 2^{c-1} buckets instead of 2^{c} (see step1)
	// we use jacobian extended formulas here as they are faster than mixed addition
	// msmProcessChunk places points into buckets base on their selector and return the weighted bucket sum in given channel
	// step 3
	// reduce the buckets weigthed sums into our result (msmReduceChunk)

	// ensure len(points) == len(scalars)
	nbPoints := len(points)
	if nbPoints != len(scalars) {
		return nil, errors.New("len(points) != len(scalars)")
	}

	// if nbTasks is not set, use all available CPUs
	if config.NbTasks <= 0 {
		config.NbTasks = runtime.NumCPU()
	} else if config.NbTasks > 1024 {
		return nil, errors.New("invalid config: config.NbTasks > 1024")
	}

	// here, we compute the best C for nbPoints
	// we split recursively until nbChunks(c) >= nbTasks,
	bestC := func(nbPoints int) uint64 {
		// implemented batchAffineMsmC methods (the c we use must be in this slice)
		implementedCs := []uint64{4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 20, 21}
		var C uint64
		// approximate cost (in group operations)
		// cost = bits/c * (nbPoints + 2^{c})
		// this needs to be verified empirically.
		// for example, on a MBP 2016, for G2 MultiExp > 8M points, hand picking c gives better results
		min := math.MaxFloat64
		for _, c := range implementedCs {
			cc := fr.Limbs * 64 * (nbPoints + (1 << (c)))
			cost := float64(cc) / float64(c)
			if cost < min {
				min = cost
				C = c
			}
		}
		// empirical, needs to be tuned.
		// if C > 16 && nbPoints < 1 << 23 {
		// 	C = 16
		// }
		return C
	}

	var C uint64
	nbSplits := 1
	nbChunks := 0
	for nbChunks < config.NbTasks {
		C = bestC(nbPoints)
		nbChunks = int(fr.Limbs * 64 / C) // number of c-bit radixes in a scalar
		if (fr.Limbs*64)%C != 0 {
			nbChunks++
		}
		nbChunks *= nbSplits
		if nbChunks < config.NbTasks {
			nbSplits <<= 1
			nbPoints >>= 1
		}
	}

	// partition the scalars
	// note: we do that before the actual chunk processing, as for each c-bit window (starting from LSW)
	// if it's larger than 2^{c-1}, we have a carry we need to propagate up to the higher window
	var smallValues int
	scalars, smallValues = partitionScalars(scalars, C, config.ScalarsMont, config.NbTasks)

	// if we have more than 10% of small values, we split the processing of the first chunk in 2
	// we may want to do that in msmInnerG2JacBatchAffine , but that would incur a cost of looping through all scalars one more time
	splitFirstChunk := (float64(smallValues) / float64(len(scalars))) >= 0.1

	// we have nbSplits intermediate results that we must sum together.
	_p := make([]G2Jac, nbSplits-1)
	chDone := make(chan int, nbSplits-1)
	for i := 0; i < nbSplits-1; i++ {
		start := i * nbPoints
		end := start + nbPoints
		go func(start, end, i int) {
			msmInnerG2JacBatchAffine(&_p[i], int(C), points[start:end], scalars[start:end], splitFirstChunk)
			chDone <- i
		}(start, end, i)
	}

	msmInnerG2JacBatchAffine(p, int(C), points[(nbSplits-1)*nbPoints:], scalars[(nbSplits-1)*nbPoints:], splitFirstChunk)
	for i := 0; i < nbSplits-1; i++ {
		done := <-chDone
		p.AddAssign(&_p[done])
	}
	close(chDone)
	return p, nil
}

func msmInnerG2JacBatchAffine(p *G2Jac, c int, points []G2Affine, scalars []fr.Element, splitFirstChunk bool) {

	switch c {

	case 4:
		p.msmC4(points, scalars, splitFirstChunk)

	case 5:
		p.msmC5(points, scalars, splitFirstChunk)

	case 6:
		p.msmC6(points, scalars, splitFirstChunk)

	case 7:
		p.msmC7(points, scalars, splitFirstChunk)

	case 8:
		p.msmC8(points, scalars, splitFirstChunk)

	case 9:
		p.msmC9(points, scalars, splitFirstChunk)

	case 10:
		p.batchAffineMsmC10(points, scalars, splitFirstChunk)

	case 11:
		p.batchAffineMsmC11(points, scalars, splitFirstChunk)

	case 12:
		p.batchAffineMsmC12(points, scalars, splitFirstChunk)

	case 13:
		p.batchAffineMsmC13(points, scalars, splitFirstChunk)

	case 14:
		p.batchAffineMsmC14(points, scalars, splitFirstChunk)

	case 15:
		p.batchAffineMsmC15(points, scalars, splitFirstChunk)

	case 16:
		p.batchAffineMsmC16(points, scalars, splitFirstChunk)

	case 20:
		p.batchAffineMsmC20(points, scalars, splitFirstChunk)

	case 21:
		p.batchAffineMsmC21(points, scalars, splitFirstChunk)

	default:
		panic("not implemented")
	}
}

// msmReduceChunkG2AffineBatchAffine reduces the weighted sum of the buckets into the result of the multiExp
func msmReduceChunkG2AffineBatchAffine(p *G2Jac, c int, chChunks []chan g2JacExtended) *G2Jac {
	var _p g2JacExtended
	totalj := <-chChunks[len(chChunks)-1]
	_p.Set(&totalj)
	for j := len(chChunks) - 2; j >= 0; j-- {
		for l := 0; l < c; l++ {
			_p.double(&_p)
		}
		totalj := <-chChunks[j]
		_p.add(&totalj)
	}

	return p.unsafeFromJacExtended(&_p)
}

type BatchG2Affine struct {
	P               [MAX_BATCH_SIZE]G2Affine
	R               [MAX_BATCH_SIZE]*G2Affine
	batchSize       int
	cptP            int
	bucketIds       map[uint32]struct{}
	buckets, points []G2Affine
}

func newBatchG2Affine(buckets, points []G2Affine) BatchG2Affine {
	batchSize := len(buckets) / 5
	if batchSize > MAX_BATCH_SIZE {
		batchSize = MAX_BATCH_SIZE
	}
	if batchSize <= 0 {
		batchSize = 1
	}
	return BatchG2Affine{
		buckets:   buckets,
		points:    points,
		batchSize: batchSize,
		bucketIds: make(map[uint32]struct{}, len(buckets)/2),
	}
}

func (b *BatchG2Affine) IsFull() bool {
	return b.cptP == b.batchSize
}

func (b *BatchG2Affine) ExecuteAndReset() {
	if b.cptP == 0 {
		return
	}
	// for i := 0; i < len(b.R); i++ {
	// 	b.R[i].Add(b.R[i], b.P[i])
	// }
	BatchAddG2Affine(b.R[:b.cptP], b.P[:b.cptP], b.cptP)
	for k := range b.bucketIds {
		delete(b.bucketIds, k)
	}
	// b.bucketIds = [MAX_BATCH_SIZE]uint32{}
	b.cptP = 0
}

func (b *BatchG2Affine) CanAdd(bID uint32) bool {
	_, ok := b.bucketIds[bID]
	return !ok
}

func (b *BatchG2Affine) Add(op batchOp) {
	// CanAdd must be called before --> ensures bucket is not "used" in current batch

	B := &b.buckets[op.bucketID]
	P := &b.points[op.pointID>>1]
	if P.IsInfinity() {
		return
	}
	// handle special cases with inf or -P / P
	if B.IsInfinity() {
		if op.isNeg() {
			B.Neg(P)
		} else {
			B.Set(P)
		}
		return
	}
	if op.isNeg() {
		// if bucket == P --> -P == 0
		if B.Equal(P) {
			B.setInfinity()
			return
		}
	} else {
		// if bucket == -P, B == 0
		if B.X.Equal(&P.X) && !B.Y.Equal(&P.Y) {
			B.setInfinity()
			return
		}
	}

	// b.bucketIds[b.cptP] = op.bucketID
	b.bucketIds[op.bucketID] = struct{}{}
	b.R[b.cptP] = B
	if op.isNeg() {
		b.P[b.cptP].Neg(P)
	} else {
		b.P[b.cptP].Set(P)
	}
	b.cptP++
}

func processQueueG2Affine(queue []batchOp, batch *BatchG2Affine) []batchOp {
	for i := len(queue) - 1; i >= 0; i-- {
		if batch.CanAdd(queue[i].bucketID) {
			batch.Add(queue[i])
			if batch.IsFull() {
				batch.ExecuteAndReset()
			}
			queue[i] = queue[len(queue)-1]
			queue = queue[:len(queue)-1]
		}
	}
	return queue

}

func msmProcessChunkG2AffineBatchAffine(chunk uint64,
	chRes chan<- g2JacExtended,
	buckets []G2Affine,
	c uint64,
	points []G2Affine,
	scalars []fr.Element) {

	mask := uint64((1 << c) - 1) // low c bits are 1
	msbWindow := uint64(1 << (c - 1))

	for i := 0; i < len(buckets); i++ {
		buckets[i].setInfinity()
	}

	jc := uint64(chunk * c)
	s := selector{}
	s.index = jc / 64
	s.shift = jc - (s.index * 64)
	s.mask = mask << s.shift
	s.multiWordSelect = (64%c) != 0 && s.shift > (64-c) && s.index < (fr.Limbs-1)
	if s.multiWordSelect {
		nbBitsHigh := s.shift - uint64(64-c)
		s.maskHigh = (1 << nbBitsHigh) - 1
		s.shiftHigh = (c - nbBitsHigh)
	}

	batch := newBatchG2Affine(buckets, points)
	queue := make([]batchOp, 0, 4096) // TODO find right capacity here.
	nbBatches := 0
	for i := 0; i < len(scalars); i++ {
		bits := (scalars[i][s.index] & s.mask) >> s.shift
		if s.multiWordSelect {
			bits += (scalars[i][s.index+1] & s.maskHigh) << s.shiftHigh
		}

		if bits == 0 {
			continue
		}

		op := batchOp{pointID: uint32(i) << 1}
		// if msbWindow bit is set, we need to substract
		if bits&msbWindow == 0 {
			// add
			op.bucketID = uint32(bits - 1)
			// buckets[bits-1].Add(&points[i], &buckets[bits-1])
		} else {
			// sub
			op.bucketID = (uint32(bits & ^msbWindow))
			op.pointID += 1
			// op.isNeg = true
			// buckets[bits & ^msbWindow].Sub( &buckets[bits & ^msbWindow], &points[i])
		}
		if batch.CanAdd(op.bucketID) {
			batch.Add(op)
			if batch.IsFull() {
				batch.ExecuteAndReset()
				nbBatches++
				if len(queue) != 0 { // TODO @gbotrel this doesn't seem to help much? should minimize queue resizing
					batch.Add(queue[len(queue)-1])
					queue = queue[:len(queue)-1]
				}
			}
		} else {
			// put it in queue.
			queue = append(queue, op)
		}
	}
	// fmt.Printf("chunk %d\nlen(queue)=%d\nnbBatches=%d\nbatchSize=%d\nnbBuckets=%d\nnbPoints=%d\n",
	// 	chunk, len(queue), nbBatches, batch.batchSize, len(buckets), len(points))
	// batch.ExecuteAndReset()
	for len(queue) != 0 {
		queue = processQueueG2Affine(queue, &batch)
		batch.ExecuteAndReset() // execute batch even if not full.
	}

	// flush items in batch.
	batch.ExecuteAndReset()

	// reduce buckets into total
	// total =  bucket[0] + 2*bucket[1] + 3*bucket[2] ... + n*bucket[n-1]

	var runningSum, total g2JacExtended
	runningSum.setInfinity()
	total.setInfinity()
	for k := len(buckets) - 1; k >= 0; k-- {
		if !buckets[k].IsInfinity() {
			runningSum.addMixed(&buckets[k])
		}
		total.add(&runningSum)
	}

	chRes <- total

}

func (p *G2Jac) batchAffineMsmC10(points []G2Affine, scalars []fr.Element, splitFirstChunk bool) *G2Jac {
	const (
		c        = 10                  // scalars partitioned into c-bit radixes
		nbChunks = (fr.Limbs * 64 / c) // number of c-bit radixes in a scalar
	)

	// for each chunk, spawn one go routine that'll loop through all the scalars in the
	// corresponding bit-window
	// note that buckets is an array allocated on the stack (for most sizes of c) and this is
	// critical for performance

	// each go routine sends its result in chChunks[i] channel
	var chChunks [nbChunks + 1]chan g2JacExtended
	for i := 0; i < len(chChunks); i++ {
		chChunks[i] = make(chan g2JacExtended, 1)
	}

	// c doesn't divide 256, last window is smaller we can allocate less buckets
	const lastC = (fr.Limbs * 64) - (c * (fr.Limbs * 64 / c))
	// TODO @gbotrel replace this in code generator
	if lastC >= 10 {
		go func(j uint64, points []G2Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]G2Affine
			msmProcessChunkG2AffineBatchAffine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	} else {
		go func(j uint64, points []G2Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]g2JacExtended
			msmProcessChunkG2Affine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	}

	processChunk := func(j int, points []G2Affine, scalars []fr.Element, chChunk chan g2JacExtended) {
		var buckets [1 << (c - 1)]G2Affine
		msmProcessChunkG2AffineBatchAffine(uint64(j), chChunk, buckets[:], c, points, scalars)
	}

	for j := int(nbChunks - 1); j > 0; j-- {
		go processChunk(j, points, scalars, chChunks[j])
	}

	if !splitFirstChunk {
		go processChunk(0, points, scalars, chChunks[0])
	} else {
		chSplit := make(chan g2JacExtended, 2)
		split := len(points) / 2
		go processChunk(0, points[:split], scalars[:split], chSplit)
		go processChunk(0, points[split:], scalars[split:], chSplit)
		go func() {
			s1 := <-chSplit
			s2 := <-chSplit
			close(chSplit)
			s1.add(&s2)
			chChunks[0] <- s1
		}()
	}

	return msmReduceChunkG2AffineBatchAffine(p, c, chChunks[:])
}

func (p *G2Jac) batchAffineMsmC11(points []G2Affine, scalars []fr.Element, splitFirstChunk bool) *G2Jac {
	const (
		c        = 11                  // scalars partitioned into c-bit radixes
		nbChunks = (fr.Limbs * 64 / c) // number of c-bit radixes in a scalar
	)

	// for each chunk, spawn one go routine that'll loop through all the scalars in the
	// corresponding bit-window
	// note that buckets is an array allocated on the stack (for most sizes of c) and this is
	// critical for performance

	// each go routine sends its result in chChunks[i] channel
	var chChunks [nbChunks + 1]chan g2JacExtended
	for i := 0; i < len(chChunks); i++ {
		chChunks[i] = make(chan g2JacExtended, 1)
	}

	// c doesn't divide 256, last window is smaller we can allocate less buckets
	const lastC = (fr.Limbs * 64) - (c * (fr.Limbs * 64 / c))
	// TODO @gbotrel replace this in code generator
	if lastC >= 10 {
		go func(j uint64, points []G2Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]G2Affine
			msmProcessChunkG2AffineBatchAffine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	} else {
		go func(j uint64, points []G2Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]g2JacExtended
			msmProcessChunkG2Affine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	}

	processChunk := func(j int, points []G2Affine, scalars []fr.Element, chChunk chan g2JacExtended) {
		var buckets [1 << (c - 1)]G2Affine
		msmProcessChunkG2AffineBatchAffine(uint64(j), chChunk, buckets[:], c, points, scalars)
	}

	for j := int(nbChunks - 1); j > 0; j-- {
		go processChunk(j, points, scalars, chChunks[j])
	}

	if !splitFirstChunk {
		go processChunk(0, points, scalars, chChunks[0])
	} else {
		chSplit := make(chan g2JacExtended, 2)
		split := len(points) / 2
		go processChunk(0, points[:split], scalars[:split], chSplit)
		go processChunk(0, points[split:], scalars[split:], chSplit)
		go func() {
			s1 := <-chSplit
			s2 := <-chSplit
			close(chSplit)
			s1.add(&s2)
			chChunks[0] <- s1
		}()
	}

	return msmReduceChunkG2AffineBatchAffine(p, c, chChunks[:])
}

func (p *G2Jac) batchAffineMsmC12(points []G2Affine, scalars []fr.Element, splitFirstChunk bool) *G2Jac {
	const (
		c        = 12                  // scalars partitioned into c-bit radixes
		nbChunks = (fr.Limbs * 64 / c) // number of c-bit radixes in a scalar
	)

	// for each chunk, spawn one go routine that'll loop through all the scalars in the
	// corresponding bit-window
	// note that buckets is an array allocated on the stack (for most sizes of c) and this is
	// critical for performance

	// each go routine sends its result in chChunks[i] channel
	var chChunks [nbChunks + 1]chan g2JacExtended
	for i := 0; i < len(chChunks); i++ {
		chChunks[i] = make(chan g2JacExtended, 1)
	}

	// c doesn't divide 256, last window is smaller we can allocate less buckets
	const lastC = (fr.Limbs * 64) - (c * (fr.Limbs * 64 / c))
	// TODO @gbotrel replace this in code generator
	if lastC >= 10 {
		go func(j uint64, points []G2Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]G2Affine
			msmProcessChunkG2AffineBatchAffine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	} else {
		go func(j uint64, points []G2Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]g2JacExtended
			msmProcessChunkG2Affine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	}

	processChunk := func(j int, points []G2Affine, scalars []fr.Element, chChunk chan g2JacExtended) {
		var buckets [1 << (c - 1)]G2Affine
		msmProcessChunkG2AffineBatchAffine(uint64(j), chChunk, buckets[:], c, points, scalars)
	}

	for j := int(nbChunks - 1); j > 0; j-- {
		go processChunk(j, points, scalars, chChunks[j])
	}

	if !splitFirstChunk {
		go processChunk(0, points, scalars, chChunks[0])
	} else {
		chSplit := make(chan g2JacExtended, 2)
		split := len(points) / 2
		go processChunk(0, points[:split], scalars[:split], chSplit)
		go processChunk(0, points[split:], scalars[split:], chSplit)
		go func() {
			s1 := <-chSplit
			s2 := <-chSplit
			close(chSplit)
			s1.add(&s2)
			chChunks[0] <- s1
		}()
	}

	return msmReduceChunkG2AffineBatchAffine(p, c, chChunks[:])
}

func (p *G2Jac) batchAffineMsmC13(points []G2Affine, scalars []fr.Element, splitFirstChunk bool) *G2Jac {
	const (
		c        = 13                  // scalars partitioned into c-bit radixes
		nbChunks = (fr.Limbs * 64 / c) // number of c-bit radixes in a scalar
	)

	// for each chunk, spawn one go routine that'll loop through all the scalars in the
	// corresponding bit-window
	// note that buckets is an array allocated on the stack (for most sizes of c) and this is
	// critical for performance

	// each go routine sends its result in chChunks[i] channel
	var chChunks [nbChunks + 1]chan g2JacExtended
	for i := 0; i < len(chChunks); i++ {
		chChunks[i] = make(chan g2JacExtended, 1)
	}

	// c doesn't divide 256, last window is smaller we can allocate less buckets
	const lastC = (fr.Limbs * 64) - (c * (fr.Limbs * 64 / c))
	// TODO @gbotrel replace this in code generator
	if lastC >= 10 {
		go func(j uint64, points []G2Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]G2Affine
			msmProcessChunkG2AffineBatchAffine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	} else {
		go func(j uint64, points []G2Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]g2JacExtended
			msmProcessChunkG2Affine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	}

	processChunk := func(j int, points []G2Affine, scalars []fr.Element, chChunk chan g2JacExtended) {
		var buckets [1 << (c - 1)]G2Affine
		msmProcessChunkG2AffineBatchAffine(uint64(j), chChunk, buckets[:], c, points, scalars)
	}

	for j := int(nbChunks - 1); j > 0; j-- {
		go processChunk(j, points, scalars, chChunks[j])
	}

	if !splitFirstChunk {
		go processChunk(0, points, scalars, chChunks[0])
	} else {
		chSplit := make(chan g2JacExtended, 2)
		split := len(points) / 2
		go processChunk(0, points[:split], scalars[:split], chSplit)
		go processChunk(0, points[split:], scalars[split:], chSplit)
		go func() {
			s1 := <-chSplit
			s2 := <-chSplit
			close(chSplit)
			s1.add(&s2)
			chChunks[0] <- s1
		}()
	}

	return msmReduceChunkG2AffineBatchAffine(p, c, chChunks[:])
}

func (p *G2Jac) batchAffineMsmC14(points []G2Affine, scalars []fr.Element, splitFirstChunk bool) *G2Jac {
	const (
		c        = 14                  // scalars partitioned into c-bit radixes
		nbChunks = (fr.Limbs * 64 / c) // number of c-bit radixes in a scalar
	)

	// for each chunk, spawn one go routine that'll loop through all the scalars in the
	// corresponding bit-window
	// note that buckets is an array allocated on the stack (for most sizes of c) and this is
	// critical for performance

	// each go routine sends its result in chChunks[i] channel
	var chChunks [nbChunks + 1]chan g2JacExtended
	for i := 0; i < len(chChunks); i++ {
		chChunks[i] = make(chan g2JacExtended, 1)
	}

	// c doesn't divide 256, last window is smaller we can allocate less buckets
	const lastC = (fr.Limbs * 64) - (c * (fr.Limbs * 64 / c))
	// TODO @gbotrel replace this in code generator
	if lastC >= 10 {
		go func(j uint64, points []G2Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]G2Affine
			msmProcessChunkG2AffineBatchAffine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	} else {
		go func(j uint64, points []G2Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]g2JacExtended
			msmProcessChunkG2Affine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	}

	processChunk := func(j int, points []G2Affine, scalars []fr.Element, chChunk chan g2JacExtended) {
		var buckets [1 << (c - 1)]G2Affine
		msmProcessChunkG2AffineBatchAffine(uint64(j), chChunk, buckets[:], c, points, scalars)
	}

	for j := int(nbChunks - 1); j > 0; j-- {
		go processChunk(j, points, scalars, chChunks[j])
	}

	if !splitFirstChunk {
		go processChunk(0, points, scalars, chChunks[0])
	} else {
		chSplit := make(chan g2JacExtended, 2)
		split := len(points) / 2
		go processChunk(0, points[:split], scalars[:split], chSplit)
		go processChunk(0, points[split:], scalars[split:], chSplit)
		go func() {
			s1 := <-chSplit
			s2 := <-chSplit
			close(chSplit)
			s1.add(&s2)
			chChunks[0] <- s1
		}()
	}

	return msmReduceChunkG2AffineBatchAffine(p, c, chChunks[:])
}

func (p *G2Jac) batchAffineMsmC15(points []G2Affine, scalars []fr.Element, splitFirstChunk bool) *G2Jac {
	const (
		c        = 15                  // scalars partitioned into c-bit radixes
		nbChunks = (fr.Limbs * 64 / c) // number of c-bit radixes in a scalar
	)

	// for each chunk, spawn one go routine that'll loop through all the scalars in the
	// corresponding bit-window
	// note that buckets is an array allocated on the stack (for most sizes of c) and this is
	// critical for performance

	// each go routine sends its result in chChunks[i] channel
	var chChunks [nbChunks + 1]chan g2JacExtended
	for i := 0; i < len(chChunks); i++ {
		chChunks[i] = make(chan g2JacExtended, 1)
	}

	// c doesn't divide 256, last window is smaller we can allocate less buckets
	const lastC = (fr.Limbs * 64) - (c * (fr.Limbs * 64 / c))
	// TODO @gbotrel replace this in code generator
	if lastC >= 10 {
		go func(j uint64, points []G2Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]G2Affine
			msmProcessChunkG2AffineBatchAffine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	} else {
		go func(j uint64, points []G2Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]g2JacExtended
			msmProcessChunkG2Affine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	}

	processChunk := func(j int, points []G2Affine, scalars []fr.Element, chChunk chan g2JacExtended) {
		var buckets [1 << (c - 1)]G2Affine
		msmProcessChunkG2AffineBatchAffine(uint64(j), chChunk, buckets[:], c, points, scalars)
	}

	for j := int(nbChunks - 1); j > 0; j-- {
		go processChunk(j, points, scalars, chChunks[j])
	}

	if !splitFirstChunk {
		go processChunk(0, points, scalars, chChunks[0])
	} else {
		chSplit := make(chan g2JacExtended, 2)
		split := len(points) / 2
		go processChunk(0, points[:split], scalars[:split], chSplit)
		go processChunk(0, points[split:], scalars[split:], chSplit)
		go func() {
			s1 := <-chSplit
			s2 := <-chSplit
			close(chSplit)
			s1.add(&s2)
			chChunks[0] <- s1
		}()
	}

	return msmReduceChunkG2AffineBatchAffine(p, c, chChunks[:])
}

func (p *G2Jac) batchAffineMsmC16(points []G2Affine, scalars []fr.Element, splitFirstChunk bool) *G2Jac {
	const (
		c        = 16                  // scalars partitioned into c-bit radixes
		nbChunks = (fr.Limbs * 64 / c) // number of c-bit radixes in a scalar
	)

	// for each chunk, spawn one go routine that'll loop through all the scalars in the
	// corresponding bit-window
	// note that buckets is an array allocated on the stack (for most sizes of c) and this is
	// critical for performance

	// each go routine sends its result in chChunks[i] channel
	var chChunks [nbChunks]chan g2JacExtended
	for i := 0; i < len(chChunks); i++ {
		chChunks[i] = make(chan g2JacExtended, 1)
	}

	processChunk := func(j int, points []G2Affine, scalars []fr.Element, chChunk chan g2JacExtended) {
		var buckets [1 << (c - 1)]G2Affine
		msmProcessChunkG2AffineBatchAffine(uint64(j), chChunk, buckets[:], c, points, scalars)
	}

	for j := int(nbChunks - 1); j > 0; j-- {
		go processChunk(j, points, scalars, chChunks[j])
	}

	if !splitFirstChunk {
		go processChunk(0, points, scalars, chChunks[0])
	} else {
		chSplit := make(chan g2JacExtended, 2)
		split := len(points) / 2
		go processChunk(0, points[:split], scalars[:split], chSplit)
		go processChunk(0, points[split:], scalars[split:], chSplit)
		go func() {
			s1 := <-chSplit
			s2 := <-chSplit
			close(chSplit)
			s1.add(&s2)
			chChunks[0] <- s1
		}()
	}

	return msmReduceChunkG2AffineBatchAffine(p, c, chChunks[:])
}

func (p *G2Jac) batchAffineMsmC20(points []G2Affine, scalars []fr.Element, splitFirstChunk bool) *G2Jac {
	const (
		c        = 20                  // scalars partitioned into c-bit radixes
		nbChunks = (fr.Limbs * 64 / c) // number of c-bit radixes in a scalar
	)

	// for each chunk, spawn one go routine that'll loop through all the scalars in the
	// corresponding bit-window
	// note that buckets is an array allocated on the stack (for most sizes of c) and this is
	// critical for performance

	// each go routine sends its result in chChunks[i] channel
	var chChunks [nbChunks + 1]chan g2JacExtended
	for i := 0; i < len(chChunks); i++ {
		chChunks[i] = make(chan g2JacExtended, 1)
	}

	// c doesn't divide 256, last window is smaller we can allocate less buckets
	const lastC = (fr.Limbs * 64) - (c * (fr.Limbs * 64 / c))
	// TODO @gbotrel replace this in code generator
	if lastC >= 10 {
		go func(j uint64, points []G2Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]G2Affine
			msmProcessChunkG2AffineBatchAffine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	} else {
		go func(j uint64, points []G2Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]g2JacExtended
			msmProcessChunkG2Affine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	}

	processChunk := func(j int, points []G2Affine, scalars []fr.Element, chChunk chan g2JacExtended) {
		var buckets [1 << (c - 1)]G2Affine
		msmProcessChunkG2AffineBatchAffine(uint64(j), chChunk, buckets[:], c, points, scalars)
	}

	for j := int(nbChunks - 1); j > 0; j-- {
		go processChunk(j, points, scalars, chChunks[j])
	}

	if !splitFirstChunk {
		go processChunk(0, points, scalars, chChunks[0])
	} else {
		chSplit := make(chan g2JacExtended, 2)
		split := len(points) / 2
		go processChunk(0, points[:split], scalars[:split], chSplit)
		go processChunk(0, points[split:], scalars[split:], chSplit)
		go func() {
			s1 := <-chSplit
			s2 := <-chSplit
			close(chSplit)
			s1.add(&s2)
			chChunks[0] <- s1
		}()
	}

	return msmReduceChunkG2AffineBatchAffine(p, c, chChunks[:])
}

func (p *G2Jac) batchAffineMsmC21(points []G2Affine, scalars []fr.Element, splitFirstChunk bool) *G2Jac {
	const (
		c        = 21                  // scalars partitioned into c-bit radixes
		nbChunks = (fr.Limbs * 64 / c) // number of c-bit radixes in a scalar
	)

	// for each chunk, spawn one go routine that'll loop through all the scalars in the
	// corresponding bit-window
	// note that buckets is an array allocated on the stack (for most sizes of c) and this is
	// critical for performance

	// each go routine sends its result in chChunks[i] channel
	var chChunks [nbChunks + 1]chan g2JacExtended
	for i := 0; i < len(chChunks); i++ {
		chChunks[i] = make(chan g2JacExtended, 1)
	}

	// c doesn't divide 256, last window is smaller we can allocate less buckets
	const lastC = (fr.Limbs * 64) - (c * (fr.Limbs * 64 / c))
	// TODO @gbotrel replace this in code generator
	if lastC >= 10 {
		go func(j uint64, points []G2Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]G2Affine
			msmProcessChunkG2AffineBatchAffine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	} else {
		go func(j uint64, points []G2Affine, scalars []fr.Element) {
			var buckets [1 << (lastC - 1)]g2JacExtended
			msmProcessChunkG2Affine(j, chChunks[j], buckets[:], c, points, scalars)
		}(uint64(nbChunks), points, scalars)
	}

	processChunk := func(j int, points []G2Affine, scalars []fr.Element, chChunk chan g2JacExtended) {
		var buckets [1 << (c - 1)]G2Affine
		msmProcessChunkG2AffineBatchAffine(uint64(j), chChunk, buckets[:], c, points, scalars)
	}

	for j := int(nbChunks - 1); j > 0; j-- {
		go processChunk(j, points, scalars, chChunks[j])
	}

	if !splitFirstChunk {
		go processChunk(0, points, scalars, chChunks[0])
	} else {
		chSplit := make(chan g2JacExtended, 2)
		split := len(points) / 2
		go processChunk(0, points[:split], scalars[:split], chSplit)
		go processChunk(0, points[split:], scalars[split:], chSplit)
		go func() {
			s1 := <-chSplit
			s2 := <-chSplit
			close(chSplit)
			s1.add(&s2)
			chChunks[0] <- s1
		}()
	}

	return msmReduceChunkG2AffineBatchAffine(p, c, chChunks[:])
}
